{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa6e0efa-5e2c-4fbb-8437-a56d5c1503f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/63] Goldey v. Fields, 606 U.S. ___ (2025)\n",
      "[2/63] Trump v. CASA, Inc., 606 U.S. ___ (2025)\n",
      "[3/63] FCC v. Consumers' Research, 606 U.S. ___ (2025)\n",
      "[4/63] Free Speech Coalition, Inc. v. Paxton, 606 U.S. ___ (2025)\n",
      "[5/63] Mahmoud v. Taylor, 606 U.S. ___ (2025)\n",
      "[6/63] Kennedy v. Braidwood Management, Inc., 606 U.S. ___ (2025)\n",
      "[7/63] Riley v. Bondi, 606 U.S. ___ (2025)\n",
      "[8/63] Medina v. Planned Parenthood South Atlantic, 606 U.S. ___ (2025)\n",
      "[9/63] Hewitt v. United States, 606 U.S. ___ (2025)\n",
      "[10/63] Gutierrez v. Saenz, 606 U.S. ___ (2025)\n",
      "[11/63] Stanley v. City of Sanford, 606 U.S. ___ (2025)\n",
      "[12/63] Fuld v. Palestine Liberation Organization, 606 U.S. ___ (2025)\n",
      "[13/63] Diamond Alternative Energy, LLC v. Environmental Protection Agency, 606 U.S. ___ (2025)\n",
      "[14/63] Esteras v. United States, 606 U.S. ___ (2025)\n",
      "[15/63] McLaughlin Chiropractic Associates, Inc. v. McKesson Corp., 606 U.S. ___ (2025)\n",
      "[16/63] Food and Drug Administration v. R.J. Reynolds Vapor Co., 606 U.S. ___ (2025)\n",
      "[17/63] Perttu v. Richards, 605 U.S. ___ (2025)\n",
      "[18/63] Nuclear Regulatory Commission v. Texas, 605 U.S. ___ (2025)\n",
      "[19/63] Environmental Protection Agency v. Calumet Shreveport Refining, L.L.C., 605 U.S. ___ (2025)\n",
      "[20/63] United States v. Skrmetti, 605 U.S. ___ (2025)\n",
      "[21/63] Oklahoma v. Environmental Protection Agency, 605 U.S. ___ (2025)\n",
      "[22/63] Rivers v. Guerrero, 605 U.S. ___ (2025)\n",
      "[23/63] Commissioner v. Zuch, 605 U.S. ___ (2025)\n",
      "[24/63] Soto v. United States, 605 U.S. ___ (2025)\n",
      "[25/63] Parrish v. United States, 605 U.S. ___ (2025)\n",
      "[26/63] A. J. T. v. Osseo Area Schools, Independent School Dist. No. 279, 605 U.S. ___ (2025)\n",
      "[27/63] Martin v. United States, 605 U.S. ___ (2025)\n",
      "[28/63] Smith & Wesson Brands, Inc. v. Estados Unidos Mexicanos, 605 U.S. ___ (2025)\n",
      "[29/63] Catholic Charities Bureau, Inc. v. Wisconsin Labor and Industry Review Commission, 605 U.S. ___ (2025)\n",
      "[30/63] CC/Devas (Mauritius) Ltd. v. Antrix Corp., 605 U.S. ___ (2025)\n",
      "[31/63] Laboratory Corp. of America Holdings v. Davis, 605 U.S. ___ (2025)\n",
      "[32/63] Ames v. Ohio Department of Youth Services, 605 U.S. ___ (2025)\n",
      "[33/63] BLOM Bank SAL v. Honickman, 605 U.S. ___ (2025)\n",
      "[34/63] Seven County Infrastructure Coalition v. Eagle County, 605 U.S. ___ (2025)\n",
      "[35/63] OK Charter School Board v. Drummond, 605 U.S. ___ (2025)\n",
      "[36/63] Kousisis v. United States, 605 U.S. ___ (2025)\n",
      "[37/63] A.A.R.P. v. Trump, 605 U.S. ___ (2025)\n",
      "[38/63] Barnes v. Felix, 605 U.S. ___ (2025)\n",
      "[39/63] Feliciano v. Department Of Transportation, 605 U.S. ___ (2025)\n",
      "[40/63] Advocate Christ Medical Center v. Kennedy, 605 U.S. ___ (2025)\n",
      "[41/63] Velazquez v. Bondi, 604 U.S. ___ (2025)\n",
      "[42/63] Cunningham v. Cornell University, 604 U.S. ___ (2025)\n",
      "[43/63] Trump v. J. G. G., 604 U.S. ___ (2025)\n",
      "[44/63] Department of Education v. California, 604 U.S. ___ (2025)\n",
      "[45/63] Medical Marijuana, Inc. v. Horn, 604 U.S. ___ (2025)\n",
      "[46/63] FDA v. Wages and White Lion Investments, LLC, 604 U.S. ___ (2025)\n",
      "[47/63] United States v. Miller, 604 U.S. ___ (2025)\n",
      "[48/63] Bondi v. Vanderstok, 604 U.S. ___ (2025)\n",
      "[49/63] Delligatti v. United States, 604 U.S. ___ (2025)\n",
      "[50/63] Thompson v. United States, 604 U.S. ___ (2025)\n",
      "[51/63] Bufkin v. Collins, 604 U.S. ___ (2025)\n",
      "[52/63] City and County of San Francisco v. EPA, 604 U.S. ___ (2025)\n",
      "[53/63] Dewberry Group, Inc. v. Dewberry Engineers Inc., 604 U.S. ___ (2025)\n",
      "[54/63] Waetzig v. Halliburton Energy Services, Inc., 604 U.S. ___ (2025)\n",
      "[55/63] Glossip v. Oklahoma, 604 U.S. ___ (2025)\n",
      "[56/63] Lackey v. Stinnie, 604 U.S. ___ (2025)\n",
      "[57/63] Hungary v. Simon, 604 U.S. ___ (2025)\n",
      "[58/63] Wisconsin Bell, Inc. v. United States ex rel. Heath, 604 U.S. ___ (2025)\n",
      "[59/63] Williams v. Reed, 604 U.S. ___ (2025)\n",
      "[60/63] Andrew v. White, 604 U.S. ___ (2025)\n",
      "[61/63] TikTok Inc. v. Garland, 604 U.S. ___ (2025)\n",
      "[62/63] Royal Canin U.S.A. v. Wullschleger, 604 U.S. ___ (2025)\n",
      "[63/63] E.M.D. Sales, Inc. v. Carrera, 604 U.S. ___ (2025)\n",
      "\n",
      "Saved 63 cases to supreme_court_2025_cases.json\n"
     ]
    }
   ],
   "source": [
    "# scrape_scotus_2025.py\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from dataclasses import asdict, dataclass, field\n",
    "from typing import List, Optional, Dict\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "BASE_URL = \"https://supreme.justia.com\"\n",
    "YEAR_PAGE = \"https://supreme.justia.com/cases/federal/us/year/2025.html\"\n",
    "OUTPUT_FILE = \"supreme_court_2025_cases.json\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
    "        \"(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "    ),\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "}\n",
    "\n",
    "# --------- Helpers ---------\n",
    "def clean_text(s: Optional[str]) -> Optional[str]:\n",
    "    \"\"\"Normalize whitespace, strip, and remove undesirable symbols/zero-width chars.\"\"\"\n",
    "    if s is None:\n",
    "        return None\n",
    "    # Replace newlines/tabs with spaces\n",
    "    s = re.sub(r\"[\\r\\n\\t]+\", \" \", s)\n",
    "    # Remove zero-width & control characters\n",
    "    s = re.sub(r\"[\\u200B-\\u200D\\uFEFF\\u2060\\u00AD]\", \"\", s)  # zero-width etc.\n",
    "    s = re.sub(r\"[\\x00-\\x1F\\x7F]\", \" \", s)\n",
    "    # Collapse multiple spaces\n",
    "    s = re.sub(r\"\\s{2,}\", \" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "def get_soup(url: str, session: requests.Session, sleep_sec: float = 0.8) -> BeautifulSoup:\n",
    "    \"\"\"Fetch a URL with polite delay and return BeautifulSoup object.\"\"\"\n",
    "    time.sleep(sleep_sec)\n",
    "    resp = session.get(url, headers=HEADERS, timeout=30)\n",
    "    # Let requests guess encoding; fallback to apparent if missing\n",
    "    if not resp.encoding:\n",
    "        resp.encoding = resp.apparent_encoding or \"utf-8\"\n",
    "    html = resp.text\n",
    "    return BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "# --------- Data Model ---------\n",
    "@dataclass\n",
    "class CaseRecord:\n",
    "    case_title: Optional[str] = None\n",
    "    docket_number: Optional[str] = None\n",
    "    court_name: Optional[str] = \"U.S. Supreme Court\"\n",
    "    date: Optional[str] = None\n",
    "    case_url: Optional[str] = None\n",
    "    justia_opinion_summary: Optional[str] = None\n",
    "    extra: Dict[str, Optional[str]] = field(default_factory=dict)\n",
    "    # Optional: legal bias analysis scaffolding (left as None for later human review)\n",
    "    legal_bias_analysis: Dict[str, Optional[str]] = field(default_factory=lambda: {\n",
    "        \"Gender Bias\": None,\n",
    "        \"Religious Bias\": None,\n",
    "        \"Racial Bias\": None,\n",
    "        \"Age Bias\": None,\n",
    "        \"Nationality Bias\": None,\n",
    "        \"Sexual Orientation Bias\": None,\n",
    "        \"Appearance Bias\": None,\n",
    "        \"Socio-Economic Status Bias\": None\n",
    "    })\n",
    "\n",
    "# --------- Parsing: Year Listing Page ---------\n",
    "def parse_year_listing(session: requests.Session) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Parse the 2025 year page to get per-case stubs:\n",
    "    title, link, docket (if present on listing), and date (if present on listing).\n",
    "    We try multiple CSS strategies to be resilient.\n",
    "    \"\"\"\n",
    "    soup = get_soup(YEAR_PAGE, session)\n",
    "    cases = []\n",
    "\n",
    "    # The page typically has a wrapper like: div.results.zebra... > div (each case)\n",
    "    containers = soup.select(\"div.results.zebra.has-negative-sides-30.-overflow-hidden > div\")\n",
    "    if not containers:\n",
    "        # Fallback: try a more generic approach\n",
    "        containers = soup.select(\"div.results div, div.results > *\")\n",
    "\n",
    "    for div in containers:\n",
    "        # We expect at least a link to the case\n",
    "        a = div.find(\"a\", href=True)\n",
    "        if not a:\n",
    "            continue\n",
    "\n",
    "        title = clean_text(a.get_text())\n",
    "        href = urljoin(BASE_URL, a[\"href\"])\n",
    "\n",
    "        # Docket number often appears in <strong> tags or text like \"Docket: 23-123\"\n",
    "        docket = None\n",
    "        strongs = div.find_all(\"strong\")\n",
    "        for st in strongs:\n",
    "            txt = clean_text(st.get_text())\n",
    "            if txt and (\"Docket\" in txt or re.search(r\"^\\d{2}-\\d+\", txt)):\n",
    "                # Either \"Docket: 23-123\" or raw \"23-123\"\n",
    "                m = re.search(r\"(\\d{2}-\\d+)\", txt)\n",
    "                docket = m.group(1) if m else txt\n",
    "                break\n",
    "        if not docket:\n",
    "            # Try inline text\n",
    "            text = clean_text(div.get_text()) or \"\"\n",
    "            m = re.search(r\"Docket[:\\s]+(\\d{2}-\\d+)\", text, flags=re.I)\n",
    "            if m:\n",
    "                docket = m.group(1)\n",
    "\n",
    "        # Date sometimes present in small/span tags\n",
    "        date = None\n",
    "        maybe_date = div.find([\"small\", \"span\"], string=re.compile(r\"\\d{4}\"))\n",
    "        if maybe_date:\n",
    "            date = clean_text(maybe_date.get_text())\n",
    "        else:\n",
    "            text = clean_text(div.get_text()) or \"\"\n",
    "            # Very forgiving date catch (Month Day, 2025)\n",
    "            m = re.search(\n",
    "                r\"(January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},\\s+2025\",\n",
    "                text\n",
    "            )\n",
    "            if m:\n",
    "                date = m.group(0)\n",
    "\n",
    "        cases.append({\n",
    "            \"title\": title,\n",
    "            \"url\": href,\n",
    "            \"docket\": docket,\n",
    "            \"date\": date\n",
    "        })\n",
    "    # De-dup if the page has ads/duplicates\n",
    "    unique = {}\n",
    "    for c in cases:\n",
    "        unique[c[\"url\"]] = c\n",
    "    return list(unique.values())\n",
    "\n",
    "# --------- Parsing: Individual Case Page ---------\n",
    "def parse_case_detail(case_url: str, session: requests.Session) -> Dict[str, Optional[str]]:\n",
    "    \"\"\"\n",
    "    Visit the case page and retrieve Justia Opinion Summary (and possibly other bits).\n",
    "    We try a few selector patterns that commonly occur across Justia pages.\n",
    "    \"\"\"\n",
    "    soup = get_soup(case_url, session)\n",
    "\n",
    "    # Try common locations for the Justia Opinion Summary\n",
    "    # 1) A heading \"Justia Opinion Summary\" followed by a paragraph or div\n",
    "    summary = None\n",
    "    heading = soup.find(lambda tag: tag.name in [\"h2\", \"h3\"] and \"Justia Opinion Summary\" in tag.get_text())\n",
    "    if heading:\n",
    "        # grab next sibling that has text\n",
    "        ns = heading.find_next_sibling()\n",
    "        while ns and clean_text(ns.get_text()) in (None, \"\"):\n",
    "            ns = ns.find_next_sibling()\n",
    "        if ns:\n",
    "            summary = clean_text(ns.get_text())\n",
    "\n",
    "    # 2) Some pages use a dedicated box/section; try a few known classes\n",
    "    if not summary:\n",
    "        summary_box = soup.select_one(\".opinion-summary, .justia-opinion-summary, section#opinion-summary, div#opinion-summary\")\n",
    "        if summary_box:\n",
    "            summary = clean_text(summary_box.get_text())\n",
    "\n",
    "    # 3) Fallback: search for a paragraph containing \"Justia Opinion Summary:\"\n",
    "    if not summary:\n",
    "        p = soup.find(\"p\", string=re.compile(r\"Justia Opinion Summary\", re.I))\n",
    "        if p:\n",
    "            summary = clean_text(p.get_text())\n",
    "\n",
    "    # Title on the detail page (sometimes more exact)\n",
    "    page_title = None\n",
    "    h1 = soup.find(\"h1\")\n",
    "    if h1:\n",
    "        page_title = clean_text(h1.get_text())\n",
    "\n",
    "    # Docket sometimes appears clearly on detail page\n",
    "    docket = None\n",
    "    text = clean_text(soup.get_text()) or \"\"\n",
    "    m = re.search(r\"Docket(?: No\\.?|):\\s*([0-9]{2}-\\d+)\", text, flags=re.I)\n",
    "    if m:\n",
    "        docket = m.group(1)\n",
    "\n",
    "    # Date sometimes present near the caption / header\n",
    "    date = None\n",
    "    # Try meta or obvious patterns\n",
    "    m2 = re.search(\n",
    "        r\"(January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},\\s+2025\",\n",
    "        text\n",
    "    )\n",
    "    if m2:\n",
    "        date = m2.group(0)\n",
    "\n",
    "    return {\n",
    "        \"page_title\": page_title,\n",
    "        \"summary\": summary,\n",
    "        \"docket\": docket,\n",
    "        \"date\": date,\n",
    "    }\n",
    "\n",
    "# --------- Main Scrape ---------\n",
    "def main():\n",
    "    session = requests.Session()\n",
    "    session.headers.update(HEADERS)\n",
    "\n",
    "    listing = parse_year_listing(session)\n",
    "    records: List[CaseRecord] = []\n",
    "\n",
    "    for idx, item in enumerate(listing, start=1):\n",
    "        url = item[\"url\"]\n",
    "        detail = parse_case_detail(url, session)\n",
    "\n",
    "        title = detail.get(\"page_title\") or item.get(\"title\")\n",
    "        docket = detail.get(\"docket\") or item.get(\"docket\")\n",
    "        date = detail.get(\"date\") or item.get(\"date\")\n",
    "\n",
    "        rec = CaseRecord(\n",
    "            case_title=title,\n",
    "            docket_number=docket,\n",
    "            date=date,\n",
    "            case_url=url,\n",
    "            # Court name fixed for this collection\n",
    "            court_name=\"U.S. Supreme Court\",\n",
    "            justia_opinion_summary=detail.get(\"summary\"),\n",
    "            extra={}\n",
    "        )\n",
    "\n",
    "        # Extra fields you might want to capture if present on listing\n",
    "        if item.get(\"title\") and item.get(\"title\") != title:\n",
    "            rec.extra[\"listing_title\"] = item.get(\"title\")\n",
    "\n",
    "        # Clean everything one more time\n",
    "        for k, v in list(asdict(rec).items()):\n",
    "            if isinstance(v, str):\n",
    "                setattr(rec, k, clean_text(v))\n",
    "            elif isinstance(v, dict):\n",
    "                # Clean dict string values\n",
    "                for dk, dv in list(v.items()):\n",
    "                    if isinstance(dv, str):\n",
    "                        v[dk] = clean_text(dv)\n",
    "\n",
    "        print(f\"[{idx}/{len(listing)}] {rec.case_title or '(no title)'}\")\n",
    "\n",
    "        records.append(rec)\n",
    "\n",
    "    # Serialize to JSON (ensure UTF-8; escape control chars removed already)\n",
    "    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump([asdict(r) for r in records], f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"\\nSaved {len(records)} cases to {OUTPUT_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db9c030-a83a-4a65-81a6-74f4fb9b4c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
